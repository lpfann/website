
@inproceedings{GopfertFeatureRelevanceBounds2017,
  address = {Bruges},
  title = {Feature {{Relevance Bounds}} for {{Linear Classification}}},
  copyright = {All rights reserved},
  abstract = {Biomedical applications often aim for an identification of relevant features for a given classification task, since these carry the promise of semantic insight into the underlying process. For correlated input dimensions, feature relevances are not unique, and the identification of meaningful subtle biomarkers remains a challenge. One approach is to identify intervals for the possible relevance of given features, a problem related to all relevant feature determination. In this contribution, we address the important case of linear classifiers and we transfer the problem how to infer feature relevance bounds to a convex optimization problem. We demonstrate the superiority of the resulting technique in comparison to popular feature-relevance determination methods in several benchmarks.},
  booktitle = {Proceedings of the {{ESANN}}},
  publisher = {{25th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning}},
  author = {Göpfert, Christina and Pfannschmidt, Lukas and Hammer, Barbara},
  year = {2017}
}

@article{GopfertInterpretationlinearclassifiers2018,
  title = {Interpretation of Linear Classifiers by Means of Feature Relevance Bounds},
  volume = {298},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2017.11.074},
  abstract = {Research on feature relevance and feature selection problems goes back several decades, but the importance of these areas continues to grow as more and more data becomes available, and machine learning methods are used to gain insight and interpret, rather than solely to solve classification or regression problems. Despite the fact that feature relevance is often discussed, it is frequently poorly defined, and the feature selection problems studied are subtly different. Furthermore, the problem of finding all features relevant for a classification problem has only recently started to gain traction, despite its importance for interpretability and integrating expert knowledge. In this paper, we attempt to unify commonly used concepts and to give an overview of the main questions and results. We formalize two interpretations of the all-relevant problem and propose a polynomial method to approximate one of them for the important hypothesis class of linear classifiers, which also enables a distinction between strongly and weakly relevant features.},
  journal = {Neurocomputing},
  author = {Göpfert, Christina and Pfannschmidt, Lukas and Göpfert, Jan Philip and Hammer, Barbara},
  month = jul,
  year = {2018},
  keywords = {Feature selection,All-Relevant,Feature Relevance,Feature Selection,Interpretability,Linear Classification,All-relevant,Feature relevance,Linear classification},
  pages = {69-79},
  file = {/home/lpfannschmidt/.zotero/zotero/xgx1jedb.default/zotero/storage/69NBXRSZ/Göpfert et al_2018_Interpretation of Linear Classifiers by Means of Feature Relevance Bounds.pdf;/home/lpfannschmidt/.zotero/zotero/xgx1jedb.default/zotero/storage/KJTTHHH7/Göpfert et al. - 2017 - Interpretation of Linear Classifiers by Means of F.pdf;/home/lpfannschmidt/.zotero/zotero/xgx1jedb.default/zotero/storage/P62CJD4Y/neurocomputing_2017_goepfert_preprint (1).pdf;/home/lpfannschmidt/.zotero/zotero/xgx1jedb.default/zotero/storage/I6XGRW56/S0925231218302169.html;/home/lpfannschmidt/.zotero/zotero/xgx1jedb.default/zotero/storage/PF4SS9BI/Göpfert et al. - 2017 - Interpretation of Linear Classifiers by Means of F.html}
}

@article{PfannschmidtFeatureRelevanceBounds2019a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1902.07662},
  title = {Feature {{Relevance Bounds}} for {{Ordinal Regression}}},
  copyright = {All rights reserved},
  abstract = {The increasing occurrence of ordinal data, mainly sociodemographic, led to a renewed research interest in ordinal regression, i.e. the prediction of ordered classes. Besides model accuracy, the interpretation of these models itself is of high relevance, and existing approaches therefore enforce e.g. model sparsity. For high dimensional or highly correlated data, however, this might be misleading due to strong variable dependencies. In this contribution, we aim for an identification of feature relevance bounds which - besides identifying all relevant features - explicitly differentiates between strongly and weakly relevant features.},
  journal = {ESANN 2019},
  author = {Pfannschmidt, Lukas and Jakob, Jonathan and Biehl, Michael and Tino, Peter and Hammer, Barbara},
  month = feb,
  year = {2019},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  file = {/home/lpfannschmidt/.zotero/zotero/xgx1jedb.default/zotero/storage/FCAKNPHN/Pfannschmidt et al. - 2019 - Feature Relevance Bounds for Ordinal Regression.pdf;/home/lpfannschmidt/.zotero/zotero/xgx1jedb.default/zotero/storage/QK455SW8/1902.html}
}

@inproceedings{PfannschmidtFRIFeatureRelevance2019a,
  title = {{{FRI}} - {{Feature Relevance Intervals}} for {{Interpretable}} and {{Interactive Data Exploration}}},
  abstract = {Most existing feature selection methods are insufficient for analytic
purposes as soon as high dimensional data or redundant sensor signals are dealt
with since features can be selected due to spurious effects or correlations
rather than causal effects. To support the finding of causal features in
biomedical experiments, we hereby present FRI, an open source Python library
that can be used to identify all-relevant variables in linear classification
and (ordinal) regression problems. Using the recently proposed feature
relevance method, FRI is able to provide the base for further general
experimentation or in specific can facilitate the search for alternative
biomarkers. It can be used in an interactive context, by providing model
manipulation and visualization methods, or in a batch process as a filter
method.},
  language = {eng},
  booktitle = {{{CIBCB}}},
  author = {Pfannschmidt, Lukas and Göpfert, Christina and Neumann, Ursula and Heider, Dominik and Hammer, Barbara},
  year = {2019},
  file = {/home/lpfannschmidt/.zotero/zotero/xgx1jedb.default/zotero/storage/VBQ9VFTX/Pfannschmidt et al. - 2019 - FRI - Feature Relevance Intervals for Interpretabl.pdf;/home/lpfannschmidt/.zotero/zotero/xgx1jedb.default/zotero/storage/EHCUUKD3/2935456.html}
}


